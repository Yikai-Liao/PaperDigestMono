id,title,abstract,year
2501.00001,Efficient Transformers,An efficient transformer variant for long sequences.,2023
2501.00002,Contrastive Learning,Contrastive approach improves representation learning.,2023
2501.00003,Robust NLP Models,Methods to make NLP models robust to noise.,2023
2501.00004,Graph Neural Nets,Scalable graph neural network techniques.,2023
2501.00005,Self-Supervised Vision,Self-supervised methods for vision pretraining.,2023
2501.00006,Lightweight LLMs,Techniques to compress large language models.,2023