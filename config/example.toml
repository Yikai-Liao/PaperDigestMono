# Minimal runtime configuration for papersys

# Legacy/general settings
# The example config is checked into config/, but real data lives under
# the repository-level data/ directory one level up.
data_root = "../data"
scheduler_enabled = true
embedding_models = ["jasper_v1"]
logging_level = "INFO"

# Ingestion configuration
[ingestion]
enabled = true
output_dir = "metadata"
curated_dir = "metadata/curated"
start_date = "2018-01-01"  # Start from 2018
end_date = ""
batch_size = 1000
max_retries = 3
retry_delay = 5.0
oai_base_url = "http://export.arxiv.org/oai2"
metadata_prefix = "arXiv"
categories = ["cs.CL", "cs.CV", "cs.AI", "cs.LG", "stat.ML"]
save_raw_responses = false

# Embedding configuration
[embedding]
enabled = true
output_dir = "embeddings"
auto_fill_backlog = true
backlog_priority = "recent_first"
max_parallel_models = 1
checkpoint_interval = 1000
upload_to_hf = false
hf_repo_id = "lyk/ArxivEmbedding"
hf_token = "env:HF_TOKEN"

[[embedding.models]]
alias = "jasper_v1"
name = "sentence-transformers/all-MiniLM-L6-v2"
dimension = 384
batch_size = 32
max_length = 512
device = ""  # Auto-detect
precision = "auto"
backend = "sentence_transformer"

[[embedding.models]]
alias = "conan_v1"
name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
dimension = 384
batch_size = 32
max_length = 512
backend = "sentence_transformer"

[[embedding.models]]
alias = "qwen3_v1"
name = "Qwen/Qwen3-Embedding-0.6B"
dimension = 768
batch_size = 8
max_length = 512
device = "cuda"  # Explicitly select GPU to avoid pre-spawn CUDA init
precision = "auto"
backend = "vllm"

# Scheduler configuration
[scheduler]
enabled = true
timezone = "Asia/Shanghai"

[scheduler.ingest_job]
enabled = true
name = "arxiv-ingestion"
cron = "0 2 * * *"  # Daily at 2 AM

[scheduler.embed_job]
enabled = true
name = "embedding-generation"
cron = "30 2 * * *"  # Daily at 2:30 AM

[scheduler.embedding_backfill_job]
enabled = true
name = "embedding-backfill"
cron = "0 4 * * 0"  # Weekly on Sunday at 4 AM

[scheduler.recommend_job]
enabled = true
name = "recommendation-pipeline"
cron = "0 5 * * *" # Daily at 5 AM local time

[scheduler.summary_job]
enabled = true
name = "summary-pipeline"
cron = "0 6 * * *" # Daily at 6 AM local time

[scheduler.backup_job]
enabled = true
name = "nightly-backup"
cron = "30 3 * * *" # Daily at 3:30 AM local time

# Backup configuration
[backup]
enabled = true
name = "nightly-backup"
sources = ["./config", "./devlog", "./papersys"]
exclude = ["*.pyc", "__pycache__/*"]
staging_dir = "./.tmp-backups"
retention = 5

[backup.destination]
storage = "local"
path = "./backups"


# Recommendation pipeline configuration
[recommend_pipeline.data]
categories = ["cs.CL", "cs.CV", "cs.AI", "cs.LG", "stat.ML"]
embedding_columns = ["jasper_v1", "conan_v1"]
preference_dir = "./preferences"
metadata_dir = "./metadata"
metadata_pattern = "metadata-*.csv"
embeddings_root = "./embeddings"
background_start_year = 2024
preference_start_year = 2023

[recommend_pipeline.trainer]
seed = 42
bg_sample_rate = 5.0

[recommend_pipeline.trainer.logistic_regression]
C = 1.0
max_iter = 1000

[recommend_pipeline.predict]
last_n_days = 7
start_date = ""
end_date = ""
high_threshold = 0.85
boundary_threshold = 0.6
sample_rate = 0.001
output_dir = "recommendations"
output_path = "predictions.parquet"
recommended_path = "recommended.parquet"
manifest_path = "manifest.json"

# Summary pipeline configuration
[summary_pipeline.pdf]
output_dir = "./pdfs"
delay = 3
max_retry = 3
fetch_latex_source = false

[summary_pipeline.llm]
model = "gemini-2.5-flash"
language = "zh"
enable_latex = true

# LLM configurations
[[llms]]
alias = "deepseek-r1"
name = "deepseek-reasoner"
base_url = "https://api.deepseek.com"
api_key = "env:DEEPSEEK_API_KEY"
temperature = 0.7
top_p = 0.8
num_workers = 10

[[llms]]
alias = "gemini-2.5-flash"
name = "gemini/gemini-2.5-flash"
base_url = ""  # LiteLLM auto-routes via model prefix
api_key = "env:GEMINI_API_KEY"
temperature = 0.1
top_p = 0.8
num_workers = 10
reasoning_effort = "high"
